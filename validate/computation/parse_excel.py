import pandas as pd
import re
from scipy.stats import pearsonr, spearmanr

def parse_nonempty_step_categories(file_path, col1, col2):
    df = pd.read_excel(file_path)

    results = []

    def extract_categories(cell):
        """ä»ä¸€ä¸ªå•å…ƒæ ¼ä¸­æå– Positive / Neutral / Negative"""
        categories = []
        if pd.isna(cell):
            return categories
        lines = str(cell).split('\n')
        for line in lines:
            match = re.search(r':\s*(Positive|Negative|Neutral)', line)
            if match:
                categories.append(match.group(1))
        return categories

    for idx, row in df.iterrows():
        cell1, cell2 = row.get(col1), row.get(col2)
        # å¦‚æœä¸¤ä¸ªéƒ½ä¸ºç©ºï¼Œè·³è¿‡
        if pd.isna(cell1) and pd.isna(cell2):
            continue

        parsed_row = {
            "row_index": idx + 2,  # åŠ 2æ˜¯å› ä¸ºExcelé€šå¸¸ä»ç¬¬2è¡Œå¼€å§‹æœ‰æ•°æ®
            col1: extract_categories(cell1),
            col2: extract_categories(cell2)
        }
        results.append(parsed_row)

    return results

def compute_scores(parsed_rows, col1, col2):
    all_scores = []

    for row in parsed_rows:
        for col in [col1, col2]:
            categories = row[col]
            if not categories:
                continue  # è·³è¿‡ç©ºçš„

            total = len(categories)
            positive = categories.count("Positive")
            neutral = categories.count("Neutral")

            validity = (positive + neutral) / total
            conciseness = neutral / total

            all_scores.append({
                "model": col,
                "row_index": row["row_index"],
                "validity_score": validity,
                "conciseness_score": conciseness
            })

    # è½¬æ¢ä¸º DataFrameï¼Œä¾¿äºè®¡ç®—å¹³å‡å€¼å’Œåç»­å±•ç¤º
    score_df = pd.DataFrame(all_scores)

    # è®¡ç®—æ¯ä¸ªæ¨¡å‹çš„å¹³å‡å¾—åˆ†
    avg_scores = score_df.groupby("model")[["validity_score", "conciseness_score"]].mean()

    return score_df, avg_scores

def compute_correlations(score_df, model1, model2):
    # åªä¿ç•™éœ€è¦çš„ä¸¤ç»„æ•°æ®
    v1 = score_df[score_df["model"] == model1].sort_values("row_index")["validity_score"].values
    v2 = score_df[score_df["model"] == model2].sort_values("row_index")["validity_score"].values

    c1 = score_df[score_df["model"] == model1].sort_values("row_index")["conciseness_score"].values
    c2 = score_df[score_df["model"] == model2].sort_values("row_index")["conciseness_score"].values

    # ç¡®ä¿å¯¹é½è¡Œæ•°
    if len(v1) != len(v2) or len(c1) != len(c2):
        raise ValueError("ä¸¤æ¨¡å‹çš„è¡Œæ•°ä¸ä¸€è‡´ï¼Œè¯·æ£€æŸ¥æ•°æ®å¯¹é½")

    # çš®å°”æ£® & æ–¯çš®å°”æ›¼ç³»æ•°
    corr_results = {
        "validity_pearson": pearsonr(v1, v2)[0],
        "validity_spearman": spearmanr(v1, v2)[0],
        "conciseness_pearson": pearsonr(c1, c2)[0],
        "conciseness_spearman": spearmanr(c1, c2)[0],
    }

    return corr_results


file_path = "merged_evaluation_output.xlsx"  # ä½ çš„Excelè·¯å¾„
# è¦åˆ†æçš„åˆ—ç»„ï¼ˆæ¨¡å‹ vs Humanï¼‰
comparison_models = ["deepseek-v3-steps", "qwen-plus-steps", "qwen-max-steps"]
col_human = "Human"

# ç»“æœæ”¶é›†
all_avg_scores = {}
all_correlations = {}

for col_model in comparison_models:
    print(f"\nğŸ“Š å¯¹æ¯”æ¨¡å‹: {col_model} vs {col_human}")

    # è§£æä¸¤åˆ—éç©ºè¡Œ
    parsed_rows = parse_nonempty_step_categories(file_path, col_model, col_human)

    # è®¡ç®—æ¯ä¸€è¡Œçš„å¾—åˆ†
    score_df, avg_scores = compute_scores(parsed_rows, col_model, col_human)
    all_avg_scores[col_model] = avg_scores

    # æ‰“å°æ¯ä¸€è¡Œå¾—åˆ†æ ·ä¾‹
    print("å„è¡Œå¾—åˆ†ç¤ºä¾‹ï¼š")
    print(score_df.head())

    # æ‰“å°å¹³å‡å¾—åˆ†
    print("\næ¨¡å‹å¹³å‡å¾—åˆ†ï¼š")
    print(avg_scores)

    # è®¡ç®—çš®å°”æ£® & æ–¯çš®å°”æ›¼
    correlations = compute_correlations(score_df, col_model, col_human)
    all_correlations[col_model] = correlations

    # æ‰“å°ç›¸å…³æ€§ç»“æœ
    print("\næ¨¡å‹å¾—åˆ†ç›¸å…³æ€§åˆ†æï¼š")
    for k, v in correlations.items():
        print(f"{k}: {v:.4f}")
