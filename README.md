# Fine-grained LLMs Evaluator

## ğŸ“Œ Introduction

**Fine-grained LLMs Evaluator** is a flexible and transparent evaluation framework designed to assess large language models (LLMs) from a **fine-grained capability perspective**. 

Currently, it supports comprehensive evaluation of the following core capabilities:

- âœï¸ **Summarization** â€” Assess the modelâ€™s ability to condense and rephrase content.
- ğŸ“… **Planning** â€” Evaluate how well the model generates coherent plans, sequences, or step-by-step solutions.
- ğŸ§® **Computation** â€” Test numerical calculation accuracy and quantitative reasoning.
- ğŸ” **Reasoning** â€” Examine the modelâ€™s logical thinking and problem-solving abilities.
- ğŸ“š **Retrieval** â€” Measure how effectively the model uses or extracts information from external sources or knowledge bases.

In addition to these built-in categories, you can easily define and integrate **custom capabilities** to tailor the evaluation to your specific needs.

## âš™ï¸ Environment Setup

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/Jonny-W12138/LLMEvaluator
cd LLMEvaluator
```

### 2ï¸âƒ£ (Optional) Create a Virtual Environment

```bash
conda create -n LLMEvaluator python=3.12
```

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

Specially, please install [BLEURT](https://github.com/google-research/bleurt) for summarization capability evaluation.

```bash
git clone https://github.com/google-research/bleurt.git
cd bleurt
pip install .
```

## ğŸ“‘ Dataset Preparation

We provide **predefined benchmark datasets** for each of the five supported capabilities: **Summarization**, **Planning**, **Computation**, **Reasoning**, and **Retrieval**. These built-in datasets help you get started quickly and ensure consistent comparisons across different models.

### ğŸ”¹ Use Built-in Datasets

By default, the evaluator comes with ready-to-use datasets for each capability. You can directly run evaluations without any additional setup.

### ğŸ”¹ Upload Custom Datasets

You can also evaluate your models with **custom datasets**:

- **Via the UI**: Use the web interface to upload new datasets.
- **Manually**: Place your dataset files in the `dataset/` directory at the project root. Organize them in subfolders (e.g., `dataset/summarization/`, `dataset/planning/`, etc.).

## âœ… Evaluation Process

The evaluation process in **Fine-grained LLMs Evaluator** is structured to guide you step-by-step, making it easy to obtain detailed insights into your modelâ€™s specific strengths and weaknesses.

### 1ï¸âƒ£ **Select Evaluation Capabilities**

From the **left sidebar**, choose which capabilities you want to evaluate.
 Available capabilities include: **Summarization**, **Planning**, **Computation**, **Reasoning**, **Retrieval**, or any **custom capabilities** you have added.

### 2ï¸âƒ£ **Create or Select an Evaluation Task**

After selecting capabilities, create a **new evaluation task** or reuse an **existing task**.
 Tasks define the configuration for your evaluation, including which model and dataset will be used.

### 3ï¸âƒ£ **Select the Evaluation Model**

Next, choose the model to evaluate. You can pick one of two options:

- **Hugging Face** â€” Select a model hosted on Hugging Face and run it locally or via an inference API.
- **API Call** â€” Provide an API key and endpoint for a remote LLM.

### 4ï¸âƒ£ **Select or Upload the Dataset**

Once the model is selected, choose the dataset to use:

- ğŸ“ **Predefined Datasets** â€” Use the built-in datasets for the selected capabilities.
- â¬†ï¸ **Upload Your Own** â€” Upload a custom dataset through the UI.
- âš™ï¸ **System-generated Data** â€” Alternatively, generate synthetic test cases automatically using the systemâ€™s built-in data generation tool.

### 5ï¸âƒ£ **Run Fine-grained Evaluation**

After selecting the dataset, follow the on-screen instructions in the UI to run the fine-grained evaluation.
 The system will execute the tasks and collect model outputs for analysis.

### 6ï¸âƒ£ **Generate an Evaluation Report**

When the evaluation is finished, you can:

- Review detailed capability scores and qualitative insights.
- Export a comprehensive evaluation report that highlights the modelâ€™s performance, strengths, and areas for improvement.
